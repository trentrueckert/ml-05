{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97a22aab",
   "metadata": {},
   "source": [
    "# Lab 5: Ensemble Machine Learning – Wine Dataset\n",
    "**Author:** Trent Rueckert\n",
    "\n",
    "**Date:** April 11, 2025\n",
    "\n",
    "**Objective:** Use different ensemble machine learning models and evaluate their performances on predicting wine quality.\n",
    "\n",
    "## Introduction\n",
    "In this notebook, I will analyze the UCI Wine Quality Dataset to predict wine quality by preparing/exploring the data, cleaning the data/handling missing values, performing feature engineering, and training classification models based on different selected features. I will be using different ensemble machine learning models and evaluating their accuracy and F1 scores.\n",
    "\n",
    "## Imports\n",
    "Import the necessary libraries with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c21919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    BaggingClassifier,\n",
    "    VotingClassifier,\n",
    ")\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d5a99",
   "metadata": {},
   "source": [
    "## Section 1. Load and Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1409e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         1599 non-null   float64\n",
      " 1   volatile acidity      1599 non-null   float64\n",
      " 2   citric acid           1599 non-null   float64\n",
      " 3   residual sugar        1599 non-null   float64\n",
      " 4   chlorides             1599 non-null   float64\n",
      " 5   free sulfur dioxide   1599 non-null   float64\n",
      " 6   total sulfur dioxide  1599 non-null   float64\n",
      " 7   density               1599 non-null   float64\n",
      " 8   pH                    1599 non-null   float64\n",
      " 9   sulphates             1599 non-null   float64\n",
      " 10  alcohol               1599 non-null   float64\n",
      " 11  quality               1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset (download from UCI and save in the same folder)\n",
    "df = pd.read_csv(\"winequality-red.csv\", sep=\";\")\n",
    "\n",
    "# Display structure and first few rows\n",
    "df.info()\n",
    "df.head()\n",
    "\n",
    "# The dataset includes 11 physicochemical input variables (features):\n",
    "# ---------------------------------------------------------------\n",
    "# - fixed acidity          mostly tartaric acid\n",
    "# - volatile acidity       mostly acetic acid (vinegar)\n",
    "# - citric acid            can add freshness and flavor\n",
    "# - residual sugar         remaining sugar after fermentation\n",
    "# - chlorides              salt content\n",
    "# - free sulfur dioxide    protects wine from microbes\n",
    "# - total sulfur dioxide   sum of free and bound forms\n",
    "# - density                related to sugar content\n",
    "# - pH                     acidity level (lower = more acidic)\n",
    "# - sulphates              antioxidant and microbial stabilizer\n",
    "# - alcohol                % alcohol by volume\n",
    "\n",
    "# The target variable is:\n",
    "# - quality (integer score from 0 to 10, rated by wine tasters)\n",
    "\n",
    "# We will simplify this target into three categories:\n",
    "#   - low (3–4), medium (5–6), high (7–8) to make classification feasible.\n",
    "#   - we will also make this numeric (we want both for clarity)\n",
    "# The dataset contains 1599 samples and 12 columns (11 features + target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c906a598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "fixed acidity           0\n",
      "volatile acidity        0\n",
      "citric acid             0\n",
      "residual sugar          0\n",
      "chlorides               0\n",
      "free sulfur dioxide     0\n",
      "total sulfur dioxide    0\n",
      "density                 0\n",
      "pH                      0\n",
      "sulphates               0\n",
      "alcohol                 0\n",
      "quality                 0\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print('Missing Values:')\n",
    "print(df.isnull().sum(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb72fdd0",
   "metadata": {},
   "source": [
    "## Section 2. Prepare the Data\n",
    "Includes cleaning, feature engineering, encoding, splitting, helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32035c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality_numeric\n",
      "1    0.824891\n",
      "2    0.135710\n",
      "0    0.039400\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Define helper function that:\n",
    "\n",
    "# Takes one input, the quality (which we will temporarily name q while in the function)\n",
    "# And returns a string of the quality label (low, medium, high)\n",
    "# This function will be used to create the quality_label column\n",
    "def quality_to_label(q):\n",
    "    if q <= 4:\n",
    "        return \"low\"\n",
    "    elif q <= 6:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "\n",
    "# Call the apply() method on the quality column to create the new quality_label column\n",
    "df[\"quality_label\"] = df[\"quality\"].apply(quality_to_label)\n",
    "\n",
    "\n",
    "# Then, create a numeric column for modeling: 0 = low, 1 = medium, 2 = high\n",
    "def quality_to_number(q):\n",
    "    if q <= 4:\n",
    "        return 0\n",
    "    elif q <= 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "\n",
    "df[\"quality_numeric\"] = df[\"quality\"].apply(quality_to_number)\n",
    "\n",
    "print(df['quality_numeric'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbec5b0",
   "metadata": {},
   "source": [
    "In the prior cell, we change the quality scores from the 1-10 scale to \"low\", \"medium\", and \"high\". Then we convert these categories to 0, 1, and 2. This is for simplicity's sake for analysis.\n",
    "\n",
    "## Section 3. Feature Selection and Justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a68aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input features (X) and target (y)\n",
    "# Features: all columns except 'quality' and 'quality_label' and 'quality_numeric' - drop these from the input array\n",
    "# Target: quality_label (the new column we just created)\n",
    "X = df.drop(columns=[\"quality\", \"quality_label\", \"quality_numeric\"])  # Features\n",
    "y = df[\"quality_numeric\"]  # Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cceeab",
   "metadata": {},
   "source": [
    "The 3 dropped features are all directly related to the target variable, thus we drop them. The target is the column we created in section 2 by changing the labels of the quallity scores. We will use all 11 of the other features because we believe that they all impact quality.\n",
    "\n",
    "## Section 4. Split the Data into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71c1b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (stratify to preserve class balance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06fa8bd",
   "metadata": {},
   "source": [
    "## Section 5.  Evaluate Model Performance (Choose 2)\n",
    "1. Random Forest (100)\n",
    "   * A strong baseline model using 100 decision trees.\n",
    "2. Random Forest (200, max_depth=10)\n",
    "    * Adds more trees, but limits tree depth to reduce overfitting.\n",
    "3. AdaBoost (100)\n",
    "   * Boosting method that focuses on correcting previous errors.\n",
    "4. AdaBoost (200, lr=0.5)\n",
    "    * More iterations and slower learning for better generalization.\n",
    "5. Gradient Boosting (100)\n",
    "    * Boosting approach using gradient descent.\n",
    "6. Voting (DT + SVM + NN)\n",
    "    * Combines diverse models by averaging their predictions.\n",
    "7. Voting (RF + LR + KNN)\n",
    "    * Another mix of different model types.\n",
    "8. Bagging (DT, 100)\n",
    "    * Builds many trees in parallel on different samples.\n",
    "9. MLP Classifier\n",
    "   * A basic neural network with one hidden layer.\n",
    "\n",
    "## Gradient Boosting (100) and Bagging (DT, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8984baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to train and evaluate models\n",
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test, results):\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred, average=\"weighted\")\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average=\"weighted\")\n",
    "\n",
    "    print(f\"\\n{name} Results\")\n",
    "    print(\"Confusion Matrix (Test):\")\n",
    "    print(confusion_matrix(y_test, y_test_pred))\n",
    "    print(f\"Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Train F1 Score: {train_f1:.4f}, Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"Model\": name,\n",
    "            \"Train Accuracy\": train_acc,\n",
    "            \"Test Accuracy\": test_acc,\n",
    "            \"Train F1\": train_f1,\n",
    "            \"Test F1\": test_f1,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c90cf74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Boosting (100) Results\n",
      "Confusion Matrix (Test):\n",
      "[[  0  13   0]\n",
      " [  3 247  14]\n",
      " [  0  16  27]]\n",
      "Train Accuracy: 0.9601, Test Accuracy: 0.8562\n",
      "Train F1 Score: 0.9584, Test F1 Score: 0.8411\n",
      "\n",
      "Bagging (DT, 100) Results\n",
      "Confusion Matrix (Test):\n",
      "[[  0  13   0]\n",
      " [  0 252  12]\n",
      " [  0  12  31]]\n",
      "Train Accuracy: 1.0000, Test Accuracy: 0.8844\n",
      "Train F1 Score: 1.0000, Test F1 Score: 0.8655\n"
     ]
    }
   ],
   "source": [
    "# Here's how to create the different types of ensemble models listed above (you don't need to do all of them yourself. \n",
    "# Choose 2 - we have a whole team working on this.)\n",
    "\n",
    "results = []\n",
    "\n",
    "# 1. Random Forest\n",
    "# evaluate_model(\n",
    "    # \"Random Forest (100)\",\n",
    "    # RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    # X_train,\n",
    "    # y_train,\n",
    "    # X_test,\n",
    "    # y_test,\n",
    "    # results,\n",
    "# )\n",
    "\n",
    "# 2. Random Forest (200, max depth=10) \n",
    "# evaluate_model(\n",
    "    # \"Random Forest (200, max_depth=10)\",\n",
    "    # RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42),\n",
    "    # X_train,\n",
    "    # y_train,\n",
    "    # X_test,\n",
    "    # y_test,\n",
    "    # results,\n",
    "# )\n",
    "\n",
    "# 3. AdaBoost \n",
    "# evaluate_model(\n",
    "    # \"AdaBoost (100)\",\n",
    "    # AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    # X_train,\n",
    "    # y_train,\n",
    "    # X_test,\n",
    "    # y_test,\n",
    "    # results,\n",
    "# )\n",
    "\n",
    "# 4. AdaBoost (200, lr=0.5) \n",
    "# evaluate_model(\n",
    "    # \"AdaBoost (200, lr=0.5)\",\n",
    "    # AdaBoostClassifier(n_estimators=200, learning_rate=0.5, random_state=42),\n",
    "    # X_train,\n",
    "    # y_train,\n",
    "    # X_test,\n",
    "    # y_test,\n",
    "    # results,\n",
    "# )\n",
    "\n",
    "# 5. Gradient Boosting\n",
    "evaluate_model(\n",
    "    \"Gradient Boosting (100)\",\n",
    "    GradientBoostingClassifier(\n",
    "        n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42\n",
    "    ),\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    results,\n",
    ")\n",
    "\n",
    "# 6. Voting Classifier (DT, SVM, NN) \n",
    "# voting1 = VotingClassifier(\n",
    "    # estimators=[\n",
    "        # (\"DT\", DecisionTreeClassifier()),\n",
    "        # (\"SVM\", SVC(probability=True)),\n",
    "        # (\"NN\", MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000)),\n",
    "    # ],\n",
    "    # voting=\"soft\",\n",
    "# )\n",
    "# evaluate_model(\n",
    "    # \"Voting (DT + SVM + NN)\", voting1, X_train, y_train, X_test, y_test, results\n",
    "# )\n",
    "\n",
    "# 7. Voting Classifier (RF, LR, KNN) \n",
    "# voting2 = VotingClassifier(\n",
    "    # estimators=[\n",
    "        # (\"RF\", RandomForestClassifier(n_estimators=100)),\n",
    "        # (\"LR\", LogisticRegression(max_iter=1000)),\n",
    "        # (\"KNN\", KNeighborsClassifier()),\n",
    "    # ],\n",
    "    # voting=\"soft\",\n",
    "# )\n",
    "# evaluate_model(\n",
    "    # \"Voting (RF + LR + KNN)\", voting2, X_train, y_train, X_test, y_test, results\n",
    "# )\n",
    "\n",
    "# 8. Bagging \n",
    "evaluate_model(\n",
    "    \"Bagging (DT, 100)\",\n",
    "    BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42\n",
    "    ),\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    results,\n",
    ")\n",
    "\n",
    "# 9. MLP Classifier \n",
    "# evaluate_model(\n",
    "    # \"MLP Classifier\",\n",
    "    # MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42),\n",
    "    # X_train,\n",
    "    # y_train,\n",
    "    # X_test,\n",
    "    # y_test,\n",
    "    # results,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3118c804",
   "metadata": {},
   "source": [
    "## Section 6. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b2e6d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of All Models:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Train F1</th>\n",
       "      <th>Test F1</th>\n",
       "      <th>Accuarcy Gap</th>\n",
       "      <th>F1 Gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bagging (DT, 100)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884375</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.865452</td>\n",
       "      <td>0.115625</td>\n",
       "      <td>0.134548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gradient Boosting (100)</td>\n",
       "      <td>0.960125</td>\n",
       "      <td>0.856250</td>\n",
       "      <td>0.95841</td>\n",
       "      <td>0.841106</td>\n",
       "      <td>0.103875</td>\n",
       "      <td>0.117304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  Train Accuracy  Test Accuracy  Train F1   Test F1  \\\n",
       "1        Bagging (DT, 100)        1.000000       0.884375   1.00000  0.865452   \n",
       "0  Gradient Boosting (100)        0.960125       0.856250   0.95841  0.841106   \n",
       "\n",
       "   Accuarcy Gap    F1 Gap  \n",
       "1      0.115625  0.134548  \n",
       "0      0.103875  0.117304  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a table of results \n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df[\"Accuarcy Gap\"] = results_df[\"Train Accuracy\"] - results_df[\"Test Accuracy\"]\n",
    "results_df[\"F1 Gap\"] = results_df[\"Train F1\"] - results_df[\"Test F1\"]\n",
    "\n",
    "results_df = results_df.sort_values(by=\"Test Accuracy\", ascending=False)\n",
    "\n",
    "print(\"\\nSummary of All Models:\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926b7b02",
   "metadata": {},
   "source": [
    "## Section 7. Conclusions and Insights\n",
    "### My Results - Gradient Boosting (100)\n",
    "* **Train Accuracy:** 96.01%\n",
    "\n",
    "    The model performs very well on the training data.\n",
    "* **Test Accuracy:** 85.63%\n",
    "    \n",
    "    The model generalizes very well to the unseen data.\n",
    "* **Accuracy Gap:** 10.39%\n",
    "\n",
    "    There are some signs of overfitting, as the data performs relatively better on the training data than the test data.\n",
    "* **Train F1:** 95.84%\n",
    "\n",
    "    The model performs excellently on the training data.\n",
    "* **Test F1:** 84.11%\n",
    "\n",
    "    The model generalizes very well overall on the unseen data, balancing the precision and recall well.\n",
    "* **F1 Gap:** 11.73%\n",
    "\n",
    "    There is some overfitting due to the significant drop off in performance from the training to the test data.\n",
    "\n",
    "### My Results - Bagging (DT, 100)\n",
    "* **Train Accuracy:** 100%\n",
    "\n",
    "    The model performs perfectly on the training data, likely overfitting.\n",
    "* **Test Accuracy:** 88.44%\n",
    "    \n",
    "    The model generalizes very well, better than the gradient boosting, but this could be due to the overfitting.\n",
    "* **Accuracy Gap:** 11.56%\n",
    "\n",
    "    The model is overfitting, as it performs much better on the training data than the test data.\n",
    "* **Train F1:** 100%\n",
    "\n",
    "    The model is a perfect predictor of the training data, which strongly suggests overfitting.\n",
    "* **Test F1:** 86.54%\n",
    "\n",
    "    The model generalizes very well overall on the unseen data, balancing the precision and recall well. It performs better on the unseen data than the gradient boosting.\n",
    "* **F1 Gap:** 13.45%\n",
    "\n",
    "    There is some overfitting due to the significant drop off in performance from the training to the test data. Even more so than the gradient boosting.\n",
    "\n",
    "### Comparing the Models\n",
    "* Bagging (DT, 100) performs better in terms of test accuracy and F1 score, but shows major signs of overfitting.\n",
    "* Gradient Boosting (100) generalizes better and is more stable, despite the lower test performance.\n",
    "\n",
    "## Comparing Other Classmates' Results\n",
    "**Binyam Ware**\n",
    "(https://github.com/bware7/applied-ml-binware/blob/main/lab05/ml05_binware.ipynb)\n",
    "* Random Forest (100):\n",
    "  * Train Accuracy: 100%\n",
    "  * Test Accuracy: 88.75%\n",
    "  * Accuracy Gap: 11.25%\n",
    "  * Train F1: 100%\n",
    "  * Test F1: 86.61%\n",
    "  * F1 Gap: 13.39%\n",
    "* Voting (DT+SVM+NN):\n",
    "  * Train Accuracy: 92.26%\n",
    "  * Test Accuracy: 86.56%\n",
    "  * Accuracy Gap: 5.70%\n",
    "  * Train F1: 90.61%\n",
    "  * Test F1: 84.34%\n",
    "  * F1 Gap: 6.26%\n",
    "\n",
    "**Craig Wilcox**\n",
    "(https://github.com/s256657/applied-ml-craigwilcox/blob/main/lab05/ensemble_craigwilcox.ipynb)\n",
    "* Random Forest (200, max_depth=10)\n",
    "  * Train Accuracy: 97.58%\n",
    "  * Test Accuracy: 88.13%\n",
    "  * Accuracy Gap: 9.45%\n",
    "  * Train F1: 97.45%\n",
    "  * Test F1: 85.96%\n",
    "  * F1 Gap: 11.49%\n",
    "\n",
    "**Justin Schroder**\n",
    "(https://github.com/SchroderJ-pixel/applied-ml-justin/blob/main/lab05/ensemble-justin.ipynb)\n",
    "* AdaBoost (100)\n",
    "  * Train Accuracy: 83.42%\n",
    "  * Test Accuracy: 82.50%\n",
    "  * Accuracy Gap: 0.92%\n",
    "  * Train F1: 82.09%\n",
    "  * Test F1: 81.58%\n",
    "  * F1 Gap: 0.51%\n",
    "\n",
    "**Ryan Krabbe**\n",
    "(https://github.com/ryankrabbe/applied-ml-krabbe/blob/main/lab05/ensemble-krabbe.ipynb)\n",
    "* Voting (RF + LR + KNN)\n",
    "  * Train Accuracy: 91.32%\n",
    "  * Test Accuracy: 85.31%\n",
    "  * Accuracy Gap: 6.01%\n",
    "  * Train F1: 89.29%\n",
    "  * Test F1: 82.10%\n",
    "  * F1 Gap: 7.19%\n",
    "\n",
    "**Brett Neely**\n",
    "(https://github.com/bncodes19/applied-ml-bneely/blob/main/lab05/ensemble-neely.ipynb)\n",
    "* MLP Classifier\n",
    "  * Train Accuracy: 85.14%\n",
    "  * Test Accuracy: 84.38%\n",
    "  * Accuracy Gap: 0.76%\n",
    "  * Train F1: 81.41%\n",
    "  * Test F1: 80.73%\n",
    "  * F1 Gap: 0.68%\n",
    "\n",
    "**Kersha Broussard**\n",
    "(https://github.com/kersha0530/ml-05/blob/main/ensemble-kbroussard.ipynb)\n",
    "* AdaBoost (200, lr=0.5)\n",
    "  * Train Accuracy: 83.97%\n",
    "  * Test Accuracy: 85.62%\n",
    "  * Accuracy Gap: 1.65%\n",
    "  * Train F1: 82.00%\n",
    "  * Test F1: 83.00%\n",
    "  * F1 Gap: 1.00%\n",
    "\n",
    "### Best Models\n",
    "I think the best models are Voting (DT+SVM+NN) and Random Forest (200, max_depth=10) due to their high test accuracies, seemingly no or less overfitting/underfitting, and low F1/Accuracy gaps. Random Forest works well because it balances bias and variance.\n",
    "Voting (DT + SVM + NN) works well because it combines diverse perspectives, reducing the chance of consistent error.\n",
    "\n",
    "### Next Steps\n",
    "If I were in a competition to make the best predictor, I would run tests with all of the models and do extensive research on each and the dataset I am working with to decide which is the best, and then go with that one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
